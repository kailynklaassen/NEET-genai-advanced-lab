{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0878a5-ec29-4da1-877d-5331737220e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Collect domain expert feedback with the Review App UI\n",
    "Check out more detail in [documentation](https://docs.databricks.com/aws/en/mlflow3/genai/human-feedback/expert-feedback/label-existing-traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a0935f-c42b-4337-a680-15cbfc121fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"mlflow[databricks]>=3.3.1\" openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbfa6c9b-3a8e-4d3c-82f7-03d120bc5865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b10dc1b6-0e35-4a5a-86a2-995bbc2a433f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Define Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3eec529-14e9-4dfd-857c-faf89948d522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow \n",
    "from databricks.sdk import WorkspaceClient\n",
    "# Let's re-use an existing experiment\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8af40302-577f-4c70-a0ec-587865305e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Define Labeling Schemas\n",
    "Labeling schemas define the questions and input types that domain experts will use to provide feedback on your traces. You can use MLflow's built-in schemas or create custom ones tailored to your specific evaluation criteria.\n",
    "\n",
    "There are two main types of labeling schemas:\n",
    "\n",
    "- Expectation Type (```type=\"expectation\"```): Used when the expert provides a \"ground truth\" or a correct answer. For example, providing the ```expected_facts``` for a RAG system's response. These labels can often be directly used in evaluation datasets.\n",
    "- Feedback Type (```type=\"feedback\"```): Used for subjective assessments, ratings, or classifications. For example, rating a response on a scale of 1-5 for politeness, or classifying if a response met certain criteria.\n",
    "\n",
    "See the [Labeling Schemas documentation](https://docs.databricks.com/aws/en/mlflow3/genai/human-feedback/concepts/labeling-schemas) to understand the various input methods for your schemas, such as categorical choices (radio buttons), numeric scales, or free-form text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186080c4-fa5c-4702-b973-26f43eb68889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from mlflow.genai.label_schemas import create_label_schema, InputCategorical, InputText\n",
    "\n",
    "# Collect feedback on the answer\n",
    "answer_quality = create_label_schema(\n",
    "    name=\"answer_quality\",\n",
    "    type=\"feedback\",\n",
    "    title=\"Is this answer concise and helpful?\",\n",
    "    input=InputCategorical(options=[\"Yes\", \"No\"]),\n",
    "    instruction=\"Please provide a rationale below.\",\n",
    "    enable_comment=True,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Collect a ground truth answer\n",
    "expected_answer = create_label_schema(\n",
    "    name=\"expected_answer\",\n",
    "    type=\"expectation\",\n",
    "    title=\"Please provide the correct answer for the user's request.\",\n",
    "    input=InputText(),\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0be32704-5687-424b-ab0b-16bc97f67212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Create a Labeling Session\n",
    "A Labeling Session is a special type of MLflow Run organizes a set of traces for review by specific experts using selected labeling schemas. It acts as a queue for the review process.\n",
    "\n",
    "See the [Labeling Session documentation](https://docs.databricks.com/aws/en/mlflow3/genai/human-feedback/concepts/labeling-sessions) for more details.\n",
    "\n",
    "Here's how to create a labeling session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d9db0ec-6d32-4fb6-b41c-c1f922d396b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai.labeling import create_labeling_session\n",
    "\n",
    "# Create the Labeling Session with the schemas we created in the previous step\n",
    "label_answers = create_labeling_session(\n",
    "    name=\"sme_label_answers\",\n",
    "    assigned_users=[], # Leaving empty for now - give permissions in Step 5\n",
    "    label_schemas=[answer_quality.name, expected_answer.name],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf98ce3e-fa8d-45d2-90f7-d0eaeee6d1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Generate traces and add to the Labeling Session\n",
    "Once your labeling session is created, you need to add traces to it. Traces are copied into the labeling session, so any labels or modifications made during the review process do not affect your original logged traces.\n",
    "\n",
    "You can add any trace in your MLflow Experiment. See the [Labeling Session documentation](https://docs.databricks.com/aws/en/mlflow3/genai/human-feedback/concepts/labeling-sessions) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1b8b9f4-cc80-4393-8911-13ba0023fd82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "mlflow.search_runs(experiment_names=[\"/Shared/genai-advanced-workshop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "992fba9f-875f-4029-b996-474a1afc3d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get most recent eval run\n",
    "runs = mlflow.search_runs(experiment_names=[\"/Shared/genai-advanced-workshop\"], filter_string=\"metrics.`safety/mean` > 0.8\", order_by=[\"start_time DESC\"], max_results=1)\n",
    "\n",
    "# Query for the traces we just generated from that run.\n",
    "# You can also paste run_id here\n",
    "traces = mlflow.search_traces(run_id=runs.run_id[0])\n",
    "# traces = mlflow.search_traces(run_id=\"3f61a91d382a489c92dba7961988f6f0\")\n",
    "\n",
    "# Add the traces to the session\n",
    "label_answers.add_traces(traces)\n",
    "\n",
    "# Print the URL to share with your domain experts\n",
    "print(f\"Share this Review App with your team: {label_answers.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90039247-a0d5-4ad3-83b5-27e317c1346b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Share the Review App with Experts\n",
    "Once your labeling session is populated with traces, you can share its URL with your domain experts. They can use this URL to access the Review App, view the traces assigned to them (or pick from unassigned ones), and provide feedback using the labeling schemas you configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf2a444f-6259-4aba-80e1-62241bb7e30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assign the feedback to the session\n",
    "label_answers.set_assigned_users([\"jiayi.wu@databricks.com\"]) # TODO: Change the email address to your user(s) or group(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "356d8360-5085-4afb-a1ff-ef4c3e7c6412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: View and Use Collected Labels\n",
    "After your domain experts have completed their reviews, the collected feedback is attached to the traces within the labeling session. You can retrieve these labels programmatically to analyze them or use them to create evaluation datasets.\n",
    "\n",
    "Labels are stored as ```Assessment``` objects on each Trace within the Labeling Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01724149-c29f-4193-9b9a-3d5ca59615a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Updated requirement for a code sample showing:\n",
    "\n",
    "# Goal: Demonstrate how to retrieve and process collected assessments (labels) from traces within a completed or in-progress labeling session.\n",
    "# Outline:\n",
    "# 1. Assume a 'labeling_session' object (from previous steps) is available.\n",
    "# 2. Get the 'mlflow_run_id' from the 'labeling_session'\n",
    "# 3. Use 'mlflow.search_traces(run_id=session_run_id)' to fetch all traces logged within that specific labeling session run.\n",
    "# 4. Iterate through the retrieved traces (e.g., rows in the DataFrame returned by search_traces).\n",
    "# 5. For each trace, access its 'assessments'. Assessments are stored as MLflow assessment objects (such as Feedback and Expectation types) in a list within the 'assessments' column of the trace.\n",
    "#    - Access assessment attributes using dot notation: assessment.name (schema name), assessment.value (expert's input), assessment.rationale (comments), assessment.source.source_id (assessor identifier), and assessment.create_time_ms (timestamp).\n",
    "# 6. Compile these assessments from all traces into a Pandas DataFrame.\n",
    "#    - The DataFrame should have columns such as: 'trace_id', 'assessment_name', 'assessment_value', 'assessment_comment', 'assessor_id', 'timestamp'.\n",
    "# 7. Print the head of the resulting DataFrame to display some of the collected labels.\n",
    "# 8. Demonstrate how to filter this DataFrame, for example, to show only assessments related to a specific schema (e.g., 'summary_quality').\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Get the experiment ID from the labeling session object\n",
    "experiment_id = label_answers.experiment_id\n",
    "print(f\"Session ID: {experiment_id}\")\n",
    "\n",
    "# Fetch all traces from the labeling session\n",
    "traces_df = mlflow.search_traces(experiment_ids=[experiment_id])\n",
    "print(f\"Found {len(traces_df)} traces\")\n",
    "\n",
    "# Extract assessments from traces\n",
    "assessments = []\n",
    "for _, trace in traces_df.iterrows():\n",
    "    if trace['assessments']:\n",
    "        for assessment in trace['assessments']:\n",
    "            if 'feedback' in assessment and pd.notna(assessment['feedback']):\n",
    "                assessments.append({\n",
    "                    'trace_id': trace['trace_id'],\n",
    "                    'assessor_id': assessment['source']['source_id'],\n",
    "                    'assessment_name': assessment['assessment_name'],\n",
    "                    'assessment_value': assessment['feedback']['value'],\n",
    "                    #'assessment_comment': assessment['rationale'],\n",
    "                    'timestamp': assessment['create_time']\n",
    "            })\n",
    "\n",
    "# Create DataFrame with all assessments\n",
    "assessments_df = pd.DataFrame(assessments)\n",
    "print(f\"\\nCollected {len(assessments_df)} assessments\")\n",
    "\n",
    "if len(assessments_df) > 0:\n",
    "    # Display the assessments\n",
    "    print(\"\\nAssessments Preview:\")\n",
    "    display(assessments_df.head())\n",
    "\n",
    "    # Filter assessments by schema name\n",
    "    summary_quality_assessments = assessments_df[\n",
    "        assessments_df['assessment_name'] == 'summary_quality']\n",
    "\n",
    "    print(f\"\\nSummary quality assessments: {len(summary_quality_assessments)} found\")\n",
    "    if not summary_quality_assessments.empty:\n",
    "        print(summary_quality_assessments[['trace_id', 'assessment_value']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7302d359-0acf-4227-aa8a-0325fc56d1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traces_df['assessments'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "853b2073-7307-437a-aab3-7d28a7b5be34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Step: Convert to Evaluation Datasets\n",
    "(Similar to the Mlflow 3.0 Evaluation Notebook.)\n",
    "\n",
    "Labels of \"expectation\" type (e.g., ```expected_summary``` from our example) are particularly useful for creating [Evaluation Datasets](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset). These datasets can then be used with ```mlflow.genai.evaluate()``` to systematically test new versions of your GenAI application against expert-defined ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd8818c-6540-4b08-ad5b-d7d059b0d054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69190818-5327-46c8-8d77-0d54d928a545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "# This table will be created in the above UC schema\n",
    "UC_PREFIX = f\"{catalog_name}.{schema_name}\"\n",
    "evaluation_dataset_table_name = f\"{UC_PREFIX}.sme_eval\"\n",
    "\n",
    "# If the evaluation dataset already exists, remove the table\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {evaluation_dataset_table_name}\")\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "    uc_table_name=f\"{evaluation_dataset_table_name}\",\n",
    ")\n",
    "print(f\"Created evaluation dataset: {evaluation_dataset_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "480f669d-e3be-4dd5-97c0-65d3f81682f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Add records to your dataset\n",
    "Follow [Approach 2: Create from domain expert labels](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset#approach-2-create-from-domain-expert-labels)\n",
    "\n",
    "Note: The documentation may not be updated depedning on when you are viewing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9990b12-56a3-4e3d-84f3-dcc4bfe349b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.genai.labeling as labeling\n",
    "\n",
    "# Get a labeling sessions\n",
    "all_sessions = labeling.get_labeling_sessions()\n",
    "print(f\"Found {len(all_sessions)} sessions\")\n",
    "\n",
    "# Sync the first session\n",
    "if len(all_sessions) > 0:\n",
    "  all_sessions[0].sync(to_dataset=f\"{evaluation_dataset_table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "06_setup_sme_review",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
