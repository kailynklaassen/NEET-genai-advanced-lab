{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75604aa3-9180-46f8-af33-0ed7d5ce6bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Initialize Catalog, Schema, Data, Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005f63da-de5d-4b7d-8495-5b69f88bd71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up Catalog, Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d387ee9-ce53-4ad7-b43d-006b4ad71f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"catalog_name_here\"\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd2e938-c5d3-4f2a-b157-9dd7f05d43b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "#import yaml\n",
    "import os\n",
    "\n",
    "# Use the workspace client to retrieve information about the current user\n",
    "w = WorkspaceClient()\n",
    "emails = w.current_user.me().emails\n",
    "user_email = next(e.value for e in emails if e.primary)\n",
    "username = user_email.split(\"@\")[0].replace(\".\", \"_\") # only letters and underscores\n",
    "print(f\"Proceed to the next cell and set the schema as {username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e32fd8c-c278-49ac-90f5-a9970e8bfad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Catalog and schema have been automatically created thanks to lab environment\n",
    "schema_name = f\"{username}\"\n",
    "\n",
    "# Create the schema if it does not already exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b2a9d0-2bbe-41a7-a3f3-de36463e6e16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- verify the catalog and schema\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01ae4f3-f846-438f-9453-d782a3014c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download data from Github to Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514973b2-2f08-46ad-803a-5372465cde7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# download from Github\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "\n",
    "# config file paths\n",
    "base_url = \"https://raw.githubusercontent.com/jiayi-wu-3150/databricks-genai-advanced-lab/main/data/\"\n",
    "csv_files = {\n",
    "    \"product_docs\": f\"{base_url}/csvs/product_docs.csv\",\n",
    "    \"customer_services\": f\"{base_url}/csvs/customer_services.csv\",\n",
    "    \"policies\": f\"{base_url}/csvs/policies.csv\", \n",
    "    \"inventories\": f\"{base_url}/csvs/inventories.csv\",\n",
    "} \n",
    "pdf_files = {\n",
    "    \"BrownBox_SwiftWatch_X500_Manual.pdf\": f\"{base_url}/pdfs/BrownBox_SwiftWatch_X500_Manual.pdf\",\n",
    "    \"SoundWave_X5_Pro_Headphones_Manual.pdf\": f\"{base_url}/pdfs/SoundWave_X5_Pro_Headphones_Manual.pdf\",\n",
    "}\n",
    "\n",
    "spark.sql(f\"CREATE VOLUME  IF NOT EXISTS `{catalog_name}`.`{schema_name}`.`raw_data`\")\n",
    "spark.sql(f\"CREATE VOLUME  IF NOT EXISTS `{catalog_name}`.`{schema_name}`.`pdfs`\")\n",
    "\n",
    "csv_dir = f\"/Volumes/{catalog_name}/{schema_name}/raw_data\"\n",
    "pdf_dir = f\"/Volumes/{catalog_name}/{schema_name}/pdfs\"\n",
    "\n",
    "# Download and load each CSV file\n",
    "for table_name, url in csv_files.items():\n",
    "    # Download CSV data\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Save original CSV bytes into the raw_file volume\n",
    "    csv_path = os.path.join(csv_dir, f\"{table_name}.csv\")\n",
    "    with open(csv_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Save CSV into pandas DataFrame, Convert to Spark DataFrame and write to table\n",
    "    df = pd.read_csv(io.StringIO(response.text))\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "    print(f\"Table {table_name} created successfully; CSV saved to {csv_path}\")\n",
    "\n",
    "for file_name, url in pdf_files.items():\n",
    "    # Download PDF data\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Save PDF to the specified volume\n",
    "    pdf_path = os.path.join(pdf_dir, file_name)\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"PDF saved successfully: {pdf_path}\")\n",
    "\n",
    "# Quick check \n",
    "display(dbutils.fs.ls(f\"/Volumes/{catalog_name}/{schema_name}/raw_data\"))\n",
    "display(dbutils.fs.ls(f\"/Volumes/{catalog_name}/{schema_name}/pdfs\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e727d339-e776-44ef-ad9c-a1500a5394dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Update the template code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb08934-8ee6-409c-91b8-6a9f6630a2e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# update code from jywu to specified schema_name\n",
    "import os\n",
    "root = os.getcwd().rsplit(\"/\", 1)[0] + '/'\n",
    "current_file = os.getcwd().rsplit(\"/\", 1)[-1]\n",
    "print(f'The following code will swap catalog and schema name in {root}, except the current file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4365943f-1f06-4899-9554-4f1182bb26a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "old_catalog = 'databricks_workshop'\n",
    "new_catalog = catalog_name\n",
    "old_schema = 'jywu'\n",
    "new_schema = schema_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c1b62c-edf7-4063-a942-0b95cee03541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# replace catalog name\n",
    "for dirpath, _, filenames in os.walk(root):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".py\", \".ipynb\")) and not filename.startswith(current_file):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            if old_catalog in content:\n",
    "                new_content = content.replace(old_catalog, new_catalog)\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(new_content)\n",
    "                print(f\"Updated: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2082e0a1-73a6-4168-956a-54e62783abb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# replace schema name\n",
    "for dirpath, _, filenames in os.walk(root):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith((\".py\", \".ipynb\")) and not filename.startswith(current_file):\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            if old_schema in content:\n",
    "                new_content = content.replace(old_schema, new_schema)\n",
    "                with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(new_content)\n",
    "                print(f\"Updated: {filepath}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4541532822327240,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "00_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
