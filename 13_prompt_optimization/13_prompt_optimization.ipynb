{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5aaa9eb5-151b-4a05-8800-68524b520b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Prompt Optimization Tutorial: Multi-Step Agent with Dataset Creation\n",
    "This notebook demonstrates an end-to-end flow to improve your prompt for a multi-step agent using the `mlflow.genai.optimize_prompt` API. In this notebook you learn to:\n",
    "- How to collect traces for LLM calls during a multi-step agent execution\n",
    "- How to create evaluation dataset from MLflow traces\n",
    "- How to run prompt optimization with your prompt, evaluation metrics and dataset\n",
    "\n",
    "[Databricks [doc](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/prompt-version-mgmt/prompt-registry/automatically-optimize-prompts) | MLflow [doc](https://mlflow.org/docs/latest/genai/prompt-registry/optimize-prompts/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af661847-904b-42d6-bc49-63cf8593399b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Agent\n",
    "The first step is defining the AI agent. In this notebook, we use LangGraph to define an agent to extract the main topic of an article. The agent consists of two LLM calls. The first LLM call summarize the long document content into a short summary and the second call extracts the main topic from the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "010e042c-0b0f-4156-a6cb-e434fc6edfc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade mlflow>=3.1.0 langchain-community langchain-openai beautifulsoup4 langgraph dspy databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "416ec9be-0e85-423b-85e8-41a3b1216b10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d0bf600-b3b5-4a27-84ad-54e74001b870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.entities import Prompt\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33dd644f-2bec-4dfb-8e8c-acd0fe6fc718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(split_docs)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "559b97fe-411c-453e-af3b-93ac871036ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Note: langchain_community.chat_models.ChatDatabricks doesn't support create_tool_calling_agent yet - it'll soon be available. Let's use ChatOpenAI for now\n",
    "llm = ChatOpenAI(\n",
    "  base_url=f\"{WorkspaceClient().config.host}/serving-endpoints/\",\n",
    "  api_key=dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get(),\n",
    "  model=\"databricks-meta-llama-3-3-70b-instruct\" \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c795af1-ae08-4ffd-9ba9-8856d5b1f72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First prompt for summarization.\n",
    "summary_prompt = mlflow.genai.register_prompt(name=f\"{catalog_name}.{schema_name}.summary_prompt\", template=\"Write a concise summary of the following:{{content}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8ae834-b21e-4cb0-8b02-78da4889b48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37586853-165d-4ba3-9aa0-766c8420442c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "summary_chain = llm | StrOutputParser()\n",
    "\n",
    "@mlflow.trace()\n",
    "def call_summary_chain(content):\n",
    "  return summary_chain.invoke([HumanMessage(summary_prompt.format(content=content))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "008c3d38-8514-4595-bcc5-7e22d15d8ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Second prompt for topic extraction.\n",
    "topic_prompt = mlflow.genai.register_prompt(name=f\"{catalog_name}.{schema_name}.topic_prompt\",\n",
    "                       template=\"\"\"\n",
    "The following is the summary:\n",
    "{{summary}}\n",
    "Extract the main topic in a few words.\n",
    "Return the response in JSON format: {\"topic\": \"...\"}\n",
    "\"\"\")\n",
    "\n",
    "topic_chain = llm | JsonOutputParser()\n",
    "\n",
    "@mlflow.trace()\n",
    "def call_topic_chain(summary):\n",
    "  return topic_chain.invoke([HumanMessage(topic_prompt.format(summary=summary))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c111321-cdbe-4db3-be3c-6d71cfae451a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(topic_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15867cb6-0984-45b9-978b-41ffe67688e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "@mlflow.trace\n",
    "def agent(content):\n",
    "  summary = call_summary_chain(content=content)\n",
    "  return call_topic_chain(summary=summary)[\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6b64d17-64a3-4787-8997-67e0e746c808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable Autologging\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af25fc0b-c267-464a-a241-3e7e925dbda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "for doc in split_docs:\n",
    "  try:\n",
    "    print(agent(doc.page_content))\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72ce5d59-3c0c-4067-b610-bf8532338afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Create an evaluation dataset from the generated traces using the `mlflow.genai.datasets` API. In this example, we focus on the second LLM call for topic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4faaead-b17d-4a96-be4d-e821f1309a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Extract the inputs and outputs of the second LLM call\n",
    "traces = mlflow.search_traces(extract_fields=[\n",
    "  \"call_topic_chain.inputs\",\n",
    "  \"call_topic_chain.outputs\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6652328f-db47-466e-b59c-698f8aaf3448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "traces.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfae58bf-16ad-4df1-882e-1362ad142a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.genai import datasets\n",
    "\n",
    "EVAL_DATASET_NAME=f\"{catalog_name}.{schema_name}.data_for_prompt_optimization\"\n",
    "dataset = datasets.create_dataset(EVAL_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f47c4e4-4800-4c98-aa25-6ff4f69b996b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataset by treating the agent outputs as the default expectations.\n",
    "traces = traces.rename(\n",
    "    columns={\n",
    "      \"call_topic_chain.inputs\": \"inputs\",\n",
    "      \"call_topic_chain.outputs\": \"expectations\",\n",
    "    }\n",
    ")[[\"inputs\", \"expectations\"]]\n",
    "traces = traces.dropna()\n",
    "dataset.merge_records(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b377b26c-ada8-4e3c-a1b1-554f0f0e8088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Labeling\n",
    "Currently, the expectation tab of the dataset contains the agent outputs. To run the prompt optimization, it's essential to have a good quality label. Go to \"evaluation\" tab -> \"dataset\" tab and modify the expectations.\n",
    "After you finish the labeling, run the following command to download the eval dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "196b02ff-1ed2-4e1e-b41a-ca9ec811ff00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = datasets.get_dataset(EVAL_DATASET_NAME)\n",
    "dataset.merge_records([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94b95d1f-5945-4653-a8a2-7591efa4cdf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.to_df()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca6cfe8-1976-4072-b642-90219d23d064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimize\n",
    "Finally, let's run `mlflow.genai.optimize_prompt` and optimize your prompt. In the code below, we use the built-in Correctness scorer as our objective function. The optimized prompt is automatically stored in the Prompt registry. Check the new prompt template after running the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ed429e-d758-4944-b477-94dcf76142c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "import mlflow\n",
    "from mlflow.genai.scorers import Correctness\n",
    "from mlflow.genai.optimize import OptimizerConfig, LLMParams\n",
    "from mlflow.genai.scorers import scorer\n",
    "\n",
    "_correctness = Correctness()\n",
    "\n",
    "@scorer\n",
    "def correctness(inputs, outputs, expectations):\n",
    "    expectations = { \"expected_response\": expectations.get(\"topic\") }\n",
    "    return _correctness(inputs=inputs, outputs=outputs, expectations=expectations).value == \"yes\"\n",
    "\n",
    "# Optimize the prompt\n",
    "result = mlflow.genai.optimize_prompt(\n",
    "    target_llm_params=LLMParams(model_name=f\"databricks/databricks-claude-3-7-sonnet\"), #target_llm_params=LLMParams(model_name=\"openai/gpt-4.1-mini\"),\n",
    "    prompt=topic_prompt,\n",
    "    train_data=dataset,\n",
    "    scorers=[correctness],\n",
    "    optimizer_config=OptimizerConfig(\n",
    "        num_instruction_candidates=8,\n",
    "        max_few_show_examples=2,\n",
    "        verbose=False, # turn it on to see the full logs\n",
    "    )\n",
    ")\n",
    "\n",
    "# The optimized prompt is automatically registered as a new version\n",
    "# Open the prompt registry web site to check the new prompt\n",
    "print(f\"The new prompt URI: {result.prompt.uri}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "13_prompt_optimization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
